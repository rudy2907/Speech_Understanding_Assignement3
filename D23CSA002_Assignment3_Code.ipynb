{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ldwnwmQFRKM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TakHemlata/SSL_Anti-spoofing.git"
      ],
      "metadata": {
        "id": "7bXRlrI92xOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89f3fdf-14b9-43d5-c942-554a8598f671"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SSL_Anti-spoofing' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/SSL_Anti-spoofing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJbzSnWTPQ5",
        "outputId": "864e82ba-5bfa-4959-c1c6-28b1a2a3e37b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch\n",
        "!pip install torch torchvision torchaudi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xkP0Nl9Fs1k",
        "outputId": "d7fa5ab5-eb71-4177-9cd7-459a4816c59d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudi (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudi\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the Fairseq repository from GitHub\n",
        "!git clone https://github.com/pytorch/fairseq.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivykx7SiF3hk",
        "outputId": "e537762a-e7b7-4aa9-af88-5c9a7b4ff16d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fairseq' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the fairseq directory\n",
        "%cd fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SIO_SmvF-AK",
        "outputId": "d706d36f-3020-4533-c76d-41c2cfb47478"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing/fairseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install fairseq\n",
        "!pip install --editable ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc3xLt95GM0s",
        "outputId": "fb6ef1dd-a729-4849-b395-69207ea15af0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/SSL_Anti-spoofing/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.10)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.12.25)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.2)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.9.2)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.11.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->fairseq==0.12.2) (12.4.127)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9409 sha256=dcf87f1e4b7aa616e728a006f5fe7933c3710c0c40d892b032823b5796d17c30\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-az907x3d/wheels/af/be/ef/9d14fa4fb28b1cbe2fc101f8c2f92ccef8b3195d4200515c5b\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 1.0.0a0+4acaa61\n",
            "    Uninstalling fairseq-1.0.0a0+4acaa61:\n",
            "      Successfully uninstalled fairseq-1.0.0a0+4acaa61\n",
            "Successfully installed fairseq-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fairseq"
      ],
      "metadata": {
        "id": "OR0BjR4EX0r4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hffk9kvWTWBQ",
        "outputId": "a20d0004-5857-4341-f18b-eae36dc69811"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --editable ./"
      ],
      "metadata": {
        "id": "7eh0jZRRTkZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a60510c-3f37-4c7b-ed7e-bd57fa346d92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (3.0.10)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2023.12.25)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (4.66.2)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.9.2)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (1.23.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+4acaa61) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+4acaa61) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+4acaa61) (4.11.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==1.0.0a0+4acaa61) (12.4.127)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==1.0.0a0+4acaa61) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==1.0.0a0+4acaa61) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==1.0.0a0+4acaa61) (1.3.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+4acaa61-0.editable-cp310-cp310-linux_x86_64.whl size=9253 sha256=25c29b8ee04b43a4ebdc14859d3f25f6eb06dbde582446a01853f60204b226c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lif9so9f/wheels/7e/e7/f3/b9dc85bed2878480a562707319b545b2d7bbf958307b8eddc5\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.12.2\n",
            "    Uninstalling fairseq-0.12.2:\n",
            "      Successfully uninstalled fairseq-0.12.2\n",
            "Successfully installed fairseq-1.0.0a0+4acaa61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fairseq"
                ]
              },
              "id": "351f96338b204bb9af03bc9f1617bbda"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHm3ncaoUS9e",
        "outputId": "dfa0bbcb-dc67-46bb-f05c-5a878a16c042"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/SSL_Anti-spoofing/\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr2_300m.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyOqsuM0V6aU",
        "outputId": "accde2d0-922a-4099-e3c9-da7644fd41ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n",
            "--2024-05-05 21:59:29--  https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr2_300m.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.34.53, 13.226.34.122, 13.226.34.7, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.34.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3808868242 (3.5G) [binary/octet-stream]\n",
            "Saving to: ‘xlsr2_300m.pt.1’\n",
            "\n",
            "xlsr2_300m.pt.1     100%[===================>]   3.55G  47.5MB/s    in 72s     \n",
            "\n",
            "2024-05-05 22:00:42 (50.1 MB/s) - ‘xlsr2_300m.pt.1’ saved [3808868242/3808868242]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srpha5jkmOKh",
        "outputId": "d0895d01-665f-4bff-a0be-bb73efa9526f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.2 in /usr/local/lib/python3.10/dist-packages (1.23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import fairseq\n",
        "\n",
        "\n",
        "___author__ = \"Hemlata Tak\"\n",
        "__email__ = \"tak@eurecom.fr\"\n",
        "\n",
        "############################\n",
        "## FOR fine-tuned SSL MODEL\n",
        "############################\n",
        "\n",
        "\n",
        "class SSLModel(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super(SSLModel, self).__init__()\n",
        "\n",
        "        cp_path = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/xlsr2_300m.pt'   # Change the pre-trained XLSR model path.\n",
        "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
        "        self.model = model[0]\n",
        "        self.device=device\n",
        "        self.out_dim = 1024\n",
        "        return\n",
        "\n",
        "    def extract_feat(self, input_data):\n",
        "\n",
        "        # put the model to GPU if it not there\n",
        "        if next(self.model.parameters()).device != input_data.device \\\n",
        "           or next(self.model.parameters()).dtype != input_data.dtype:\n",
        "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
        "            self.model.train()\n",
        "\n",
        "\n",
        "        if True:\n",
        "            # input should be in shape (batch, length)\n",
        "            if input_data.ndim == 3:\n",
        "                input_tmp = input_data[:, :, 0]\n",
        "            else:\n",
        "                input_tmp = input_data\n",
        "\n",
        "            # [batch, length, dim]\n",
        "            emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
        "        return emb\n",
        "\n",
        "\n",
        "#---------AASIST back-end------------------------#\n",
        "''' Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu and Nicholas Evans.\n",
        "    AASIST: Audio Anti-Spoofing Using Integrated Spectro-Temporal Graph Attention Networks.\n",
        "    In Proc. ICASSP 2022, pp: 6367--6371.'''\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # attention map\n",
        "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
        "        self.att_weight = self._init_new_params(out_dim, 1)\n",
        "\n",
        "        # project\n",
        "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # batch norm\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "        # dropout for inputs\n",
        "        self.input_drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # activate\n",
        "        self.act = nn.SELU(inplace=True)\n",
        "\n",
        "        # temperature\n",
        "        self.temp = 1.\n",
        "        if \"temperature\" in kwargs:\n",
        "            self.temp = kwargs[\"temperature\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x   :(#bs, #node, #dim)\n",
        "        '''\n",
        "        # apply input dropout\n",
        "        x = self.input_drop(x)\n",
        "\n",
        "        # derive attention map\n",
        "        att_map = self._derive_att_map(x)\n",
        "\n",
        "        # projection\n",
        "        x = self._project(x, att_map)\n",
        "\n",
        "        # apply batch norm\n",
        "        x = self._apply_BN(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "    def _pairwise_mul_nodes(self, x):\n",
        "        '''\n",
        "        Calculates pairwise multiplication of nodes.\n",
        "        - for attention map\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, #dim)\n",
        "        '''\n",
        "\n",
        "        nb_nodes = x.size(1)\n",
        "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
        "        x_mirror = x.transpose(1, 2)\n",
        "\n",
        "        return x * x_mirror\n",
        "\n",
        "    def _derive_att_map(self, x):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = self._pairwise_mul_nodes(x)\n",
        "        # size: (#bs, #node, #node, #dim_out)\n",
        "        att_map = torch.tanh(self.att_proj(att_map))\n",
        "        # size: (#bs, #node, #node, 1)\n",
        "        att_map = torch.matmul(att_map, self.att_weight)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _project(self, x, att_map):\n",
        "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
        "        x2 = self.proj_without_att(x)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _apply_BN(self, x):\n",
        "        org_size = x.size()\n",
        "        x = x.view(-1, org_size[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(org_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_new_params(self, *size):\n",
        "        out = nn.Parameter(torch.FloatTensor(*size))\n",
        "        nn.init.xavier_normal_(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HtrgGraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
        "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
        "\n",
        "        # attention map\n",
        "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
        "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
        "\n",
        "        # project\n",
        "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # batch norm\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "        # dropout for inputs\n",
        "        self.input_drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # activate\n",
        "        self.act = nn.SELU(inplace=True)\n",
        "\n",
        "        # temperature\n",
        "        self.temp = 1.\n",
        "        if \"temperature\" in kwargs:\n",
        "            self.temp = kwargs[\"temperature\"]\n",
        "\n",
        "    def forward(self, x1, x2, master=None):\n",
        "        '''\n",
        "        x1  :(#bs, #node, #dim)\n",
        "        x2  :(#bs, #node, #dim)\n",
        "        '''\n",
        "        #print('x1',x1.shape)\n",
        "        #print('x2',x2.shape)\n",
        "        num_type1 = x1.size(1)\n",
        "        num_type2 = x2.size(1)\n",
        "        #print('num_type1',num_type1)\n",
        "        #print('num_type2',num_type2)\n",
        "        x1 = self.proj_type1(x1)\n",
        "        #print('proj_type1',x1.shape)\n",
        "        x2 = self.proj_type2(x2)\n",
        "        #print('proj_type2',x2.shape)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        #print('Concat x1 and x2',x.shape)\n",
        "\n",
        "        if master is None:\n",
        "            master = torch.mean(x, dim=1, keepdim=True)\n",
        "            #print('master',master.shape)\n",
        "        # apply input dropout\n",
        "        x = self.input_drop(x)\n",
        "\n",
        "        # derive attention map\n",
        "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
        "        #print('master',master.shape)\n",
        "        # directional edge for master node\n",
        "        master = self._update_master(x, master)\n",
        "        #print('master',master.shape)\n",
        "        # projection\n",
        "        x = self._project(x, att_map)\n",
        "        #print('proj x',x.shape)\n",
        "        # apply batch norm\n",
        "        x = self._apply_BN(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x1 = x.narrow(1, 0, num_type1)\n",
        "        #print('x1',x1.shape)\n",
        "        x2 = x.narrow(1, num_type1, num_type2)\n",
        "        #print('x2',x2.shape)\n",
        "        return x1, x2, master\n",
        "\n",
        "    def _update_master(self, x, master):\n",
        "\n",
        "        att_map = self._derive_att_map_master(x, master)\n",
        "        master = self._project_master(x, master, att_map)\n",
        "\n",
        "        return master\n",
        "\n",
        "    def _pairwise_mul_nodes(self, x):\n",
        "        '''\n",
        "        Calculates pairwise multiplication of nodes.\n",
        "        - for attention map\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, #dim)\n",
        "        '''\n",
        "\n",
        "        nb_nodes = x.size(1)\n",
        "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
        "        x_mirror = x.transpose(1, 2)\n",
        "\n",
        "        return x * x_mirror\n",
        "\n",
        "    def _derive_att_map_master(self, x, master):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = x * master\n",
        "        att_map = torch.tanh(self.att_projM(att_map))\n",
        "\n",
        "        att_map = torch.matmul(att_map, self.att_weightM)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _derive_att_map(self, x, num_type1, num_type2):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = self._pairwise_mul_nodes(x)\n",
        "        # size: (#bs, #node, #node, #dim_out)\n",
        "        att_map = torch.tanh(self.att_proj(att_map))\n",
        "        # size: (#bs, #node, #node, 1)\n",
        "\n",
        "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
        "\n",
        "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
        "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
        "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
        "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
        "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
        "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
        "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
        "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
        "\n",
        "        att_map = att_board\n",
        "\n",
        "\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _project(self, x, att_map):\n",
        "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
        "        x2 = self.proj_without_att(x)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _project_master(self, x, master, att_map):\n",
        "\n",
        "        x1 = self.proj_with_attM(torch.matmul(\n",
        "            att_map.squeeze(-1).unsqueeze(1), x))\n",
        "        x2 = self.proj_without_attM(master)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _apply_BN(self, x):\n",
        "        org_size = x.size()\n",
        "        x = x.view(-1, org_size[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(org_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_new_params(self, *size):\n",
        "        out = nn.Parameter(torch.FloatTensor(*size))\n",
        "        nn.init.xavier_normal_(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GraphPool(nn.Module):\n",
        "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.proj = nn.Linear(in_dim, 1)\n",
        "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
        "        self.in_dim = in_dim\n",
        "\n",
        "    def forward(self, h):\n",
        "        Z = self.drop(h)\n",
        "        weights = self.proj(Z)\n",
        "        scores = self.sigmoid(weights)\n",
        "        new_h = self.top_k_graph(scores, h, self.k)\n",
        "\n",
        "        return new_h\n",
        "\n",
        "    def top_k_graph(self, scores, h, k):\n",
        "        \"\"\"\n",
        "        args\n",
        "        =====\n",
        "        scores: attention-based weights (#bs, #node, 1)\n",
        "        h: graph data (#bs, #node, #dim)\n",
        "        k: ratio of remaining nodes, (float)\n",
        "        returns\n",
        "        =====\n",
        "        h: graph pool applied data (#bs, #node', #dim)\n",
        "        \"\"\"\n",
        "        _, n_nodes, n_feat = h.size()\n",
        "        n_nodes = max(int(n_nodes * k), 1)\n",
        "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
        "        idx = idx.expand(-1, -1, n_feat)\n",
        "\n",
        "        h = h * scores\n",
        "        h = torch.gather(h, 1, idx)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Residual_block(nn.Module):\n",
        "    def __init__(self, nb_filts, first=False):\n",
        "        super().__init__()\n",
        "        self.first = first\n",
        "\n",
        "        if not self.first:\n",
        "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
        "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
        "                               out_channels=nb_filts[1],\n",
        "                               kernel_size=(2, 3),\n",
        "                               padding=(1, 1),\n",
        "                               stride=1)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
        "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
        "                               out_channels=nb_filts[1],\n",
        "                               kernel_size=(2, 3),\n",
        "                               padding=(0, 1),\n",
        "                               stride=1)\n",
        "\n",
        "        if nb_filts[0] != nb_filts[1]:\n",
        "            self.downsample = True\n",
        "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
        "                                             out_channels=nb_filts[1],\n",
        "                                             padding=(0, 1),\n",
        "                                             kernel_size=(1, 3),\n",
        "                                             stride=1)\n",
        "\n",
        "        else:\n",
        "            self.downsample = False\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if not self.first:\n",
        "            out = self.bn1(x)\n",
        "            out = self.selu(out)\n",
        "        else:\n",
        "            out = x\n",
        "\n",
        "        #print('out',out.shape)\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        #print('aft conv1 out',out.shape)\n",
        "        out = self.bn2(out)\n",
        "        out = self.selu(out)\n",
        "        # print('out',out.shape)\n",
        "        out = self.conv2(out)\n",
        "        #print('conv2 out',out.shape)\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.conv_downsample(identity)\n",
        "\n",
        "        out += identity\n",
        "        #out = self.mp(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # AASIST parameters\n",
        "        filts = [128, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
        "        gat_dims = [64, 32]\n",
        "        pool_ratios = [0.5, 0.5, 0.5, 0.5]\n",
        "        temperatures =  [2.0, 2.0, 100.0, 100.0]\n",
        "\n",
        "\n",
        "        ####\n",
        "        # create network wav2vec 2.0\n",
        "        ####\n",
        "        self.ssl_model = SSLModel(self.device)\n",
        "        self.LL = nn.Linear(self.ssl_model.out_dim, 128)\n",
        "\n",
        "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
        "        self.first_bn1 = nn.BatchNorm2d(num_features=64)\n",
        "        self.drop = nn.Dropout(0.5, inplace=True)\n",
        "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "\n",
        "        # RawNet2 encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(1,1)),\n",
        "            nn.SELU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 64, kernel_size=(1,1)),\n",
        "\n",
        "        )\n",
        "        # position encoding\n",
        "        self.pos_S = nn.Parameter(torch.randn(1, 42, filts[-1][-1]))\n",
        "\n",
        "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
        "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
        "\n",
        "        # Graph module\n",
        "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
        "                                               gat_dims[0],\n",
        "                                               temperature=temperatures[0])\n",
        "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
        "                                               gat_dims[0],\n",
        "                                               temperature=temperatures[1])\n",
        "        # HS-GAL layer\n",
        "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
        "\n",
        "        # Graph pooling layers\n",
        "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
        "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
        "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "\n",
        "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "\n",
        "        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #-------pre-trained Wav2vec model fine tunning ------------------------##\n",
        "        x_ssl_feat = self.ssl_model.extract_feat(x.squeeze(-1))\n",
        "        x = self.LL(x_ssl_feat) #(bs,frame_number,feat_out_dim)\n",
        "\n",
        "        # post-processing on front-end features\n",
        "        x = x.transpose(1, 2)   #(bs,feat_out_dim,frame_number)\n",
        "        x = x.unsqueeze(dim=1) # add channel\n",
        "        x = F.max_pool2d(x, (3, 3))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.selu(x)\n",
        "\n",
        "        # RawNet2-based encoder\n",
        "        x = self.encoder(x)\n",
        "        x = self.first_bn1(x)\n",
        "        x = self.selu(x)\n",
        "\n",
        "        w = self.attention(x)\n",
        "\n",
        "        #------------SA for spectral feature-------------#\n",
        "        w1 = F.softmax(w,dim=-1)\n",
        "        m = torch.sum(x * w1, dim=-1)\n",
        "        e_S = m.transpose(1, 2) + self.pos_S\n",
        "\n",
        "        # graph module layer\n",
        "        gat_S = self.GAT_layer_S(e_S)\n",
        "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
        "\n",
        "        #------------SA for temporal feature-------------#\n",
        "        w2 = F.softmax(w,dim=-2)\n",
        "        m1 = torch.sum(x * w2, dim=-2)\n",
        "\n",
        "        e_T = m1.transpose(1, 2)\n",
        "\n",
        "        # graph module layer\n",
        "        gat_T = self.GAT_layer_T(e_T)\n",
        "        out_T = self.pool_T(gat_T)\n",
        "\n",
        "        # learnable master node\n",
        "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
        "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
        "\n",
        "        # inference 1\n",
        "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
        "            out_T, out_S, master=self.master1)\n",
        "\n",
        "        out_S1 = self.pool_hS1(out_S1)\n",
        "        out_T1 = self.pool_hT1(out_T1)\n",
        "\n",
        "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
        "            out_T1, out_S1, master=master1)\n",
        "        out_T1 = out_T1 + out_T_aug\n",
        "        out_S1 = out_S1 + out_S_aug\n",
        "        master1 = master1 + master_aug\n",
        "\n",
        "        # inference 2\n",
        "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
        "            out_T, out_S, master=self.master2)\n",
        "        out_S2 = self.pool_hS2(out_S2)\n",
        "        out_T2 = self.pool_hT2(out_T2)\n",
        "\n",
        "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
        "            out_T2, out_S2, master=master2)\n",
        "        out_T2 = out_T2 + out_T_aug\n",
        "        out_S2 = out_S2 + out_S_aug\n",
        "        master2 = master2 + master_aug\n",
        "\n",
        "        out_T1 = self.drop_way(out_T1)\n",
        "        out_T2 = self.drop_way(out_T2)\n",
        "        out_S1 = self.drop_way(out_S1)\n",
        "        out_S2 = self.drop_way(out_S2)\n",
        "        master1 = self.drop_way(master1)\n",
        "        master2 = self.drop_way(master2)\n",
        "\n",
        "        out_T = torch.max(out_T1, out_T2)\n",
        "        out_S = torch.max(out_S1, out_S2)\n",
        "        master = torch.max(master1, master2)\n",
        "\n",
        "        # Readout operation\n",
        "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
        "        T_avg = torch.mean(out_T, dim=1)\n",
        "\n",
        "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
        "        S_avg = torch.mean(out_S, dim=1)\n",
        "\n",
        "        last_hidden = torch.cat(\n",
        "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
        "\n",
        "        last_hidden = self.drop(last_hidden)\n",
        "        output = self.out_layer(last_hidden)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "7zmLzweDUSQw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(device)"
      ],
      "metadata": {
        "id": "euvBjoFAYWYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cd54fc-54b2-4179-91d5-c3272df61118"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(4,64600)\n",
        "output = model(input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E823bcUZrqf",
        "outputId": "912e766b-9b00-4dff-d9fc-baece5904fa8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3440, -1.8508],\n",
            "        [ 1.4543,  0.7977],\n",
            "        [-0.9376,  2.0270],\n",
            "        [-0.5219,  0.3217]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "ofvjs6G4aFtX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp_path = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/xlsr2_300m.pt'\n",
        "ssl_model = SSLModel(device)"
      ],
      "metadata": {
        "id": "1hRLzuMW5nfS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ssl_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fIwikiLA7YK",
        "outputId": "b7bbec74-e37d-41d6-d8ab-3b417e412271"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSLModel(\n",
            "  (model): Wav2Vec2Model(\n",
            "    (feature_extractor): ConvFeatureExtractionModel(\n",
            "      (conv_layers): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "          (2): Sequential(\n",
            "            (0): TransposeLast()\n",
            "            (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (2): TransposeLast()\n",
            "          )\n",
            "          (3): GELU(approximate='none')\n",
            "        )\n",
            "        (1-4): 4 x Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "          (2): Sequential(\n",
            "            (0): TransposeLast()\n",
            "            (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (2): TransposeLast()\n",
            "          )\n",
            "          (3): GELU(approximate='none')\n",
            "        )\n",
            "        (5-6): 2 x Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "          (2): Sequential(\n",
            "            (0): TransposeLast()\n",
            "            (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (2): TransposeLast()\n",
            "          )\n",
            "          (3): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (dropout_input): Dropout(p=0.0, inplace=False)\n",
            "    (dropout_features): Dropout(p=0.0, inplace=False)\n",
            "    (quantizer): GumbelVectorQuantizer(\n",
            "      (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
            "    )\n",
            "    (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (encoder): TransformerEncoder(\n",
            "      (pos_conv): Sequential(\n",
            "        (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
            "        (1): SamePad()\n",
            "        (2): GELU(approximate='none')\n",
            "      )\n",
            "      (layers): ModuleList(\n",
            "        (0-23): 24 x TransformerSentenceEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "          (dropout3): Dropout(p=0.0, inplace=False)\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (final_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchaudio import load"
      ],
      "metadata": {
        "id": "hOZHwwtIA9Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Traverse the root directory to collect file paths\n",
        "        for class_name in os.listdir(root_dir):\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            file_names = os.listdir(class_dir)\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_dir, file_name)\n",
        "                self.file_paths.append(file_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.file_paths[idx]\n",
        "\n",
        "        # Load audio waveform\n",
        "        waveform, sample_rate = load(audio_path)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "x8SuruMiCLOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "# Define a transform to resample audio to a common sample rate\n",
        "transform = Compose([\n",
        "    Resample(orig_freq=44100, new_freq=16000)  # Adjust the sample rates as needed\n",
        "])\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = SpeechDataset(root_dir='/content/drive/MyDrive/Dataset_Speech_Assignment/Dataset_Speech_Assignment', transform=transform)\n",
        "\n",
        "# Access an example\n",
        "waveform = dataset[0]\n",
        "\n",
        "# Accessing the length of the dataset\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H95tU4DkCmcZ",
        "outputId": "083b609a-8665-4fcd-f9d2-05d2a3731ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_workers = 4  # Adjust based on your system configuration\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "yqyjYPm1CoQh",
        "outputId": "0c73a598-eb7b-49ec-be35-cae35035f753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DataLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3e9d1328701b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# Adjust based on your system configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(device).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTeIJQqVDCh4",
        "outputId": "39b5f137-9e2d-42bb-a9d3-582b54e2ba88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCXGUumvDdWQ",
        "outputId": "b773f188-1fc1-4059-961f-354a08951054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (ssl_model): SSLModel(\n",
            "    (model): Wav2Vec2Model(\n",
            "      (feature_extractor): ConvFeatureExtractionModel(\n",
            "        (conv_layers): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): Sequential(\n",
            "              (0): TransposeLast()\n",
            "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (2): TransposeLast()\n",
            "            )\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "          (1-4): 4 x Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): Sequential(\n",
            "              (0): TransposeLast()\n",
            "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (2): TransposeLast()\n",
            "            )\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "          (5-6): 2 x Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): Sequential(\n",
            "              (0): TransposeLast()\n",
            "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (2): TransposeLast()\n",
            "            )\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (dropout_input): Dropout(p=0.0, inplace=False)\n",
            "      (dropout_features): Dropout(p=0.0, inplace=False)\n",
            "      (quantizer): GumbelVectorQuantizer(\n",
            "        (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
            "      )\n",
            "      (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (encoder): TransformerEncoder(\n",
            "        (pos_conv): Sequential(\n",
            "          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
            "          (1): SamePad()\n",
            "          (2): GELU(approximate='none')\n",
            "        )\n",
            "        (layers): ModuleList(\n",
            "          (0-23): 24 x TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (final_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (LL): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (first_bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (first_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (drop): Dropout(p=0.5, inplace=True)\n",
            "  (drop_way): Dropout(p=0.2, inplace=True)\n",
            "  (selu): SELU(inplace=True)\n",
            "  (encoder): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Residual_block(\n",
            "        (conv1): Conv2d(1, 32, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (selu): SELU(inplace=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
            "        (conv_downsample): Conv2d(1, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Residual_block(\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (selu): SELU(inplace=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Residual_block(\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(32, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (selu): SELU(inplace=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
            "        (conv_downsample): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Residual_block(\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (selu): SELU(inplace=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Residual_block(\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (selu): SELU(inplace=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Residual_block(\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (selu): SELU(inplace=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (attention): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): SELU(inplace=True)\n",
            "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (GAT_layer_S): GraphAttentionLayer(\n",
            "    (att_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (proj_with_att): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (proj_without_att): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (input_drop): Dropout(p=0.2, inplace=False)\n",
            "    (act): SELU(inplace=True)\n",
            "  )\n",
            "  (GAT_layer_T): GraphAttentionLayer(\n",
            "    (att_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (proj_with_att): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (proj_without_att): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (input_drop): Dropout(p=0.2, inplace=False)\n",
            "    (act): SELU(inplace=True)\n",
            "  )\n",
            "  (HtrgGAT_layer_ST11): HtrgGraphAttentionLayer(\n",
            "    (proj_type1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (proj_type2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (att_proj): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (att_projM): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_with_att): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_without_att): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_with_attM): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_without_attM): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (input_drop): Dropout(p=0.2, inplace=False)\n",
            "    (act): SELU(inplace=True)\n",
            "  )\n",
            "  (HtrgGAT_layer_ST12): HtrgGraphAttentionLayer(\n",
            "    (proj_type1): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_type2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (att_proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (att_projM): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_with_att): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_without_att): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_with_attM): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_without_attM): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (input_drop): Dropout(p=0.2, inplace=False)\n",
            "    (act): SELU(inplace=True)\n",
            "  )\n",
            "  (HtrgGAT_layer_ST21): HtrgGraphAttentionLayer(\n",
            "    (proj_type1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (proj_type2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (att_proj): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (att_projM): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_with_att): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_without_att): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_with_attM): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (proj_without_attM): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (input_drop): Dropout(p=0.2, inplace=False)\n",
            "    (act): SELU(inplace=True)\n",
            "  )\n",
            "  (HtrgGAT_layer_ST22): HtrgGraphAttentionLayer(\n",
            "    (proj_type1): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_type2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (att_proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (att_projM): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_with_att): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_without_att): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_with_attM): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (proj_without_attM): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (input_drop): Dropout(p=0.2, inplace=False)\n",
            "    (act): SELU(inplace=True)\n",
            "  )\n",
            "  (pool_S): GraphPool(\n",
            "    (sigmoid): Sigmoid()\n",
            "    (proj): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (drop): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (pool_T): GraphPool(\n",
            "    (sigmoid): Sigmoid()\n",
            "    (proj): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (drop): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (pool_hS1): GraphPool(\n",
            "    (sigmoid): Sigmoid()\n",
            "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (drop): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (pool_hT1): GraphPool(\n",
            "    (sigmoid): Sigmoid()\n",
            "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (drop): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (pool_hS2): GraphPool(\n",
            "    (sigmoid): Sigmoid()\n",
            "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (drop): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (pool_hT2): GraphPool(\n",
            "    (sigmoid): Sigmoid()\n",
            "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (drop): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (out_layer): Linear(in_features=160, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 10  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in data_loader:\n",
        "        inputs = batch[0].to(device)  # Move input data to the device (CPU or GPU)\n",
        "        # Assuming you have labels (which you don't, but I'll include it for completeness)\n",
        "        labels = batch[1].to(device)  # Move labels to the device\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(data_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgTfsSaDUoib",
        "outputId": "30668b43-28ed-49c6-cc82-67b73467416f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-262-59ddf1363229>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move input data to the device (CPU or GPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Assuming you have labels (which you don't, but I'll include it for completeness)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move labels to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "McIy2sAmz-UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, fraction=0.01):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Iterate through the directories to collect file paths and generate labels\n",
        "        for dirpath, _, filenames in os.walk(root_dir):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                self.file_paths.append(file_path)\n",
        "                # Generate label based on directory name (0 for Fake, 1 for Real)\n",
        "                label = 0 if 'Fake' in dirpath else 1\n",
        "                self.labels.append(label)\n",
        "\n",
        "        # Randomly sample a fraction of the dataset\n",
        "        sample_size = int(len(self.file_paths) * fraction)\n",
        "        sampled_indices = np.random.choice(len(self.file_paths), sample_size, replace=False)\n",
        "        self.file_paths = [self.file_paths[i] for i in sampled_indices]\n",
        "        self.labels = [self.labels[i] for i in sampled_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Load audio file and apply preprocessing\n",
        "        waveform, _ = librosa.load(file_path, sr=16000)\n",
        "        waveform = process_audio(waveform)  # Implement your preprocessing function\n",
        "\n",
        "        # Apply transformation if available\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, label"
      ],
      "metadata": {
        "id": "enaKGzbINtH7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "Z7h-3_S6OA31"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset instance with only 10% of the data\n",
        "root_dir = \"/content/drive/MyDrive/for-2sec/for-2seconds\"\n",
        "dataset = CustomAudioDataset(root_dir, fraction=0.01)"
      ],
      "metadata": {
        "id": "fqPQc0zoOe_M"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "12wNuHY5OkBN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for batching and shuffling\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "r62vgfj7OmWa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "IY4bUfGcOrdY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "Br_S-zj3O3PW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch_idx, (waveforms, labels) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss or other metrics at the end of each epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "75aV6lpjPJjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the model"
      ],
      "metadata": {
        "id": "ryvPyXSR9Y9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n"
      ],
      "metadata": {
        "id": "Wso9S_X38u3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model_dir = \"/content/drive/MyDrive\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, \"model.pt\")\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "-Omjf7vN8dyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for waveforms, labels in data_loader:\n",
        "        outputs = model(waveforms)\n",
        "        predictions = torch.sigmoid(outputs)  # Apply sigmoid activation for binary classification\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_predictions.extend(predictions.numpy())"
      ],
      "metadata": {
        "id": "woJ58ge28nDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(all_labels, all_predictions)\n",
        "\n",
        "# Calculate EER score\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
        "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "print(f\"EER Score: {eer}\")"
      ],
      "metadata": {
        "id": "53c7vCWI8qaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the eer and auc for custom dataset based on the saved model"
      ],
      "metadata": {
        "id": "BLzN7MFL9R2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()  # Instantiate your model\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "IIR19X-m9Nma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store labels and predictions\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    for waveforms in data_loader:\n",
        "        outputs = model(waveforms.to(device))\n",
        "        predictions = torch.sigmoid(outputs).cpu().numpy()  # Apply sigmoid activation for binary classification\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(all_labels, all_predictions)\n",
        "\n",
        "# Calculate EER score\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
        "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "print(f\"EER Score: {eer}\")"
      ],
      "metadata": {
        "id": "YvtZ59Ou9Pl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}